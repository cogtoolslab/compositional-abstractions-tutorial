{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8c362",
   "metadata": {},
   "source": [
    "# Linguistic Analyses for Compositional Abstractions\n",
    "## Notebook 1: Exploring abstraction in language about building\n",
    "\n",
    "Preparation of this notebook was led by [Zoe Tait](https://www.linkedin.com/in/zoetait/), in collaboration with [Will McCarthy](https://wpmccarthy.com/) and [Robert Hawkins](https://rdhawkins.com/). \n",
    "\n",
    "These results were originally reported in: \n",
    "[McCarthy*, W., Hawkins*, R., Wang, H., Holdaway, C., and Fan, J. (2021). Learning to communicate about shared procedural abstractions. Proceedings of the 43rd Annual Meeting of the Cognitive Science Society.](https://cogtoolslab.github.io/pdf/mccarthy_cogsci_2021b.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfa2a0",
   "metadata": {},
   "source": [
    "### *NOTE: THIS NOTEBOOK SERVES AS THE INSTRUCTOR VERSION*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6cd56",
   "metadata": {},
   "source": [
    "This notebook specifically looks at linguistic analysis and is dvided into two sections, where each section has multiple parts.  \n",
    "**Section 1** looks at whether or not people become more efficient in their language usage over time.  \n",
    "**Section 2** looks at in what ways one might become more efficient in their language usage over time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9eed8",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will begin by loading in the required packages. Please run the following code.\n",
    "\n",
    "*Note that you may be missing certain packages. Be sure to install these missing packages before proceeding. Some particular packages that you may be missing include the `NLTK` package (which can be accessed here: https://www.nltk.org/data.html) and the `num2words` package.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this code to load the required packages\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib, io\n",
    "os.getcwd()\n",
    "\n",
    "import numpy as np\n",
    "# import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# from PIL import Image, ImageOps, ImageDraw, ImageFont \n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "import num2words\n",
    "from num2words import num2words\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3eb9b",
   "metadata": {},
   "source": [
    "## Section 1: Do people become more efficient in their language usage over time? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554eb38b",
   "metadata": {},
   "source": [
    "### Part 1.0: Preview the Data\n",
    "\n",
    "In **Part 1.0** we will preview the data from the paper by reading in two different dataframes that contain the results from the experiment then examining the structure of them. \n",
    "\n",
    "*This section is just a preview and it will not include any exercises that need to be completed. While reading this section, begin to think about what questions you may be interested in asking about this data to specifically investigate if people become more efficient in their language usage over time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa1008",
   "metadata": {},
   "source": [
    "Here is more information about the two dataframes:\n",
    "- `df_chat`: a dataframe that includes our data and the text instructions from the architects that we are interested in examining\n",
    "- `df_trial`: a dataframe that includes data on the builder participant accuracy in addition to the text instructions sent by the architexts we are interested in examining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to read in the two different dataframes\n",
    "df_chat = pd.read_csv('../data/df_chat.csv')\n",
    "df_trial = pd.read_csv('../data/df_trial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial = df_trial[df_trial.repNum!=\"practice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74e166",
   "metadata": {},
   "source": [
    "Here, are removing data that didn't meet the accuracy threshold (75% accuracy) as a basic preprocessing step before beginning any analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data where there is 75% Accuracy on 75% of trials\n",
    "df75 = pd.DataFrame(df_trial.groupby(['gameid', 'trialNum'])['trialScore'].sum()>75).groupby(['gameid']).sum()\n",
    "df75['trials'] = df75['trialScore']\n",
    "\n",
    "df75 = df75[df75['trials']>=9]\n",
    "includedGames = list(df75.reset_index().gameid)\n",
    "\n",
    "print(\"Total dyads achieving 75% Accuracy on 75% of trials:\",len(df75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the data that did not meet the threshold from analysis\n",
    "df_chat = df_chat[df_chat.gameid.isin(includedGames)]\n",
    "df_trial = df_trial[df_trial.gameid.isin(includedGames)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0ce92",
   "metadata": {},
   "source": [
    "Now, we will preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd54912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a preview of the df_chat dataframe\n",
    "df_chat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342319ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a preview of the df_trial dataframe\n",
    "df_trial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444407b",
   "metadata": {},
   "source": [
    "### Part 1.1: Analyzing the Efficiency of Language Usage Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1764ee",
   "metadata": {},
   "source": [
    "In **Part 1.1**, we will be conducting some basic linguistic analyses to understand more about our data and the architects' text instructions within data. We are curious about how people's language usage changes throughout the time course of the task. Specifically, how language usage becomes more efficient. \n",
    "\n",
    "One way to go about examining how this may happen is by obtaining a better understanding of both the **word count** and **character count** of the text instructions that the architects sent to the builder. It might be particularly useful examine how these counts changed across trials or across time.\n",
    "\n",
    "Is there a column in one of the two dataframes that we previously examined that contains information about **word counts** and/or **character counts**? Let us take a further look. \n",
    "\n",
    "\n",
    "*Hint: We can also create columns if these do not already exist. Once we have these counts, it may be important to further investigate the amount of text instructions were sent by the architexts. Consider this when writing your code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c213a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# create a column for word count\n",
    "df_chat['word_count'] = df_chat['content'].str.split(' ').str.len()\n",
    "\n",
    "# create a column for character count\n",
    "df_chat['char_count'] = df_chat['content'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70903fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# df_chat[\"timeElapsedInTurn\"] = pd.to_numeric(df_chat['timeElapsedInTurn'])\n",
    "\n",
    "# add word count and character count to df_trial dataframe\n",
    "trial_sums = df_chat[['gameid','trialNum','word_count','char_count']].groupby(['gameid','trialNum']).sum().reset_index()\n",
    "df_trial = df_trial.merge(trial_sums, how='outer',on=['gameid','trialNum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# message counts\n",
    "counts = df_chat.groupby(['gameid','trialNum'])[['iterationName']].count().reset_index()\\\n",
    "    .rename(columns={'iterationName':'n_messages'})\n",
    "df_trial = df_trial.merge(counts, how='left', on=['gameid','trialNum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea04ca",
   "metadata": {},
   "source": [
    "### Part 1.2: Visualizing Word Count and Character Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec9d1e",
   "metadata": {},
   "source": [
    "We have now conducted some basic linguistic analyses to understand more about the potential efficiency in language usage by the architects. \n",
    "\n",
    "Now, in **Part 1.2**,  the goal is to further understand this concept by visualizing the analyses we just completed. The goal is to create a plot that looks something like the following (*which visualizes word count*) for both word and character count, visualizing the mean count(s) so that we can understand how these counts change across time.\n",
    "\n",
    "<p style=\"font-size: smaller;  text-align: center;\">\n",
    "  <img width=\"40%\" src=\"../img/word_count.png\" style=\"\"></img> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910fc54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# define the key columns of the df_trial dataframe that are needed \n",
    "# to plot the average word & character count across repetition number\n",
    "key_cols = ['word_count', 'char_count', 'repNum']\n",
    "\n",
    "# group by repNum and calculate means\n",
    "repNum_grouped = df_trial[key_cols].groupby('repNum')\n",
    "repNum_grouped_means = repNum_grouped.mean()\n",
    "\n",
    "# print the average word & character account as a table\n",
    "print(repNum_grouped_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed042b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "plt.plot(repNum_grouped_means['word_count'], linestyle='-', marker='o', markersize=10)\n",
    "plt.xlabel('repetition')\n",
    "plt.ylabel('words')\n",
    "#plt.xlim(0.5, 4.5)\n",
    "plt.ylim(0, 70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# plot char count over repetitions\n",
    "plt.plot(repNum_grouped_means['char_count'], linestyle='-', marker='o', markersize=10)\n",
    "plt.xlabel('repetition')\n",
    "plt.ylabel('characters')\n",
    "#plt.xlim(0.75, 4.25)\n",
    "plt.ylim(0, 350)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999c868",
   "metadata": {},
   "source": [
    "## Section 2: In what ways might one become more efficient in their language usage over time?\n",
    "\n",
    "While our analyses so far tell us about language use in general, we're primarily interested in how the expressions used to refer to certain entities (*the text instructions*) in our experiment change over time. We have now seen that the participants become more efficient in their language usage over time, but why is that? In particular, we want to know when people transition from providing instructions about lower-level, block by block placements, to higher-level tower abstractions. While we could in principle use NLP techniques to extract noun phrases and assess their meaning, people might use a wide variety of expressions to refer to blocks and towers.  \n",
    "\n",
    "Because of this, we asked naive raters to **identify the referring expressions** used in each message, as well as the **number of abstractions at each level (block vs. tower)**. In **Section 2**, we will analyze these referring expressions to understand why the participants are becoming more efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5d56d",
   "metadata": {},
   "source": [
    "### Part 2.0: Preview the Data\n",
    "\n",
    "In **Part 2.0** we will preview the data from the paper by reading in one dataframe that looks at the referring expression annotations that indicate whether people were transitioning from providing lower-level versus higher-level tower abstractiond. These annotations were completed by the naive raters. We will then examine the structure of this dataframe.\n",
    "\n",
    "*This section is just a preview and it will not include any exercises that need to be completed. While reading this section, begin to think about what questions you may be interested in asking about this data to specifically investigate in what ways might one become more efficient in their language usage over time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to read in the referring expressions dataframe\n",
    "df_ref_exps = pd.read_csv('../data/df_ref_exps.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afaeca9",
   "metadata": {},
   "source": [
    "Here is a preview of the `df_ref_exps` dataframe, which includes the referring expression annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to preview the df_ref_exps dataframe\n",
    "df_ref_exps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de37d4",
   "metadata": {},
   "source": [
    "Here, we preview the referring expressions that were identified by the raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48815539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code looks at the collection of referring expressions identified by raters\n",
    "df_ref_exps[['message','content','block','tower']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2598e0",
   "metadata": {},
   "source": [
    "Here we are converting the `content` column to be of type \"string.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605249ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code to converts the content column to be type string\n",
    "df_ref_exps.loc[:,'content'] = df_ref_exps.loc[:,'content'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7128b",
   "metadata": {},
   "source": [
    "### Part 2.2: Data Processing\n",
    "\n",
    "Because we are interested in understanding in what ways people become more efficient with their language usage over time, we must further analyze the words included in the text instructions from the architects. To do this, we need to process some of the data in the `df_ref_exps` dataframe. \n",
    "\n",
    "To process this data, one step to consider is to remove **(1) stop words**, as they do not make a difference to our analyses. We also want to consider **(2) number words**, for example converting the word \"two\" to the number \"2\". We also want to consider **(3) lemmatizing words**, which means obtaining the stem of the word.\n",
    "\n",
    "*It is important to note that there exists NLP processing libraries that can be used when preprocessing data. To execute the 3 goals listed above, we will be using `NLTK`, which stands for \"Natural Language Toolkit.\" We loaded this package in the beginning of the notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "## STEP 1\n",
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df_ref_exps['content'] = df_ref_exps['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_ref_exps['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01bc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "## STEP 2\n",
    "# convert number words\n",
    "\n",
    "def num_2_words(sentence):\n",
    "    out = \"\"\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            o = num2words(word)\n",
    "        except:\n",
    "            o = word\n",
    "        out = out+\" \"+ o\n",
    "    return out\n",
    "\n",
    "df_ref_exps['content'] = df_ref_exps['content'].apply(lambda x: num_2_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45624918",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "## STEP 3\n",
    "# lemmatize\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokenizer.tokenize(text)]\n",
    "\n",
    "df_ref_exps['BOW_lemmatized'] = df_ref_exps['content'].apply(lemmatize_text)\n",
    "df_ref_exps['BOW_lemmatized'] = df_ref_exps['BOW_lemmatized'].apply(lambda x: [i.upper() for i in x])\n",
    "\n",
    "df_ref_exps[['message','content','BOW_lemmatized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "df_ref_exps['word_freq'] = df_ref_exps['BOW_lemmatized'].apply(lambda x: Counter(x))\n",
    "df_ref_exps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6580c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "df_ref_exps['BOW_concat'] = df_ref_exps['BOW_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bcc81",
   "metadata": {},
   "source": [
    "### Part 2.3: Creating Distributions of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e31946",
   "metadata": {},
   "source": [
    "In Part **2.3**, now that we have preprocessed the dataframe, the goal is to get to the root of our question in that ways in which people become more efficient in their language usage. To do this, it is important to consider the distribution of their words throughout the text instructions and how this distribution may change over time. \n",
    "\n",
    "*Hint: Consider caclulating proportions of words and seeing how this changes.*\n",
    "\n",
    "Our final goal is to plot this data to understand in what ways the participants become more efficient throughout trials, with a plot that looks something like the following:\n",
    "\n",
    "<p style=\"font-size: smaller;  text-align: center;\">\n",
    "  <img width=\"40%\" src=\"../img/change_in_proportion.png\" style=\"\"></img> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# **2.3.1** Here, our goal is to convert the word frequencies to proportions. \n",
    "# Currently, the word counts represent the counts from all 4 of our naive raters. \n",
    "# So that we can examine how frequently different words were used, we need to convert these values into proportions.\n",
    "split_words = df_ref_exps['BOW_concat'].apply(lambda x: x.split())\n",
    "all_words = list(pd.Series([st for row in split_words for st in row]).unique())\n",
    "support = {}\n",
    "for word in all_words:\n",
    "    support[word] = 0.000000001\n",
    "    \n",
    "def get_pdist(row):\n",
    "    num_words = np.sum(list(row['word_freq'].values()))\n",
    "    pdist = support.copy()\n",
    "    for i, (word, count) in enumerate(row['word_freq'].items()):\n",
    "        pdist[word] = count/num_words\n",
    "    return pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "df_ref_exps['word_pdist'] = df_ref_exps.apply(get_pdist, axis = 1)\n",
    "df_ref_exps['word_pdist_numeric'] = df_ref_exps['word_pdist'].apply(lambda dist: list(dist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc069ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "df_all_words = df_ref_exps[['dyad_gameid', 'rep', 'BOW_concat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505c025",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "for w in all_words:\n",
    "    df_all_words.loc[:,w] = df_all_words['BOW_concat'].apply(lambda row: int(w in row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1554e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "df_all_words_reps = df_all_words.groupby('rep').agg(sum)\n",
    "df_all_words_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# examine the change in word frequencies between trials.\n",
    "# prep data\n",
    "df_ref_exps_rep = df_ref_exps.groupby('rep')['BOW_concat'].apply(lambda group:' '.join(group)).reset_index()\n",
    "df_ref_exps_rep['word_freq'] = df_ref_exps_rep['BOW_concat'].apply(lambda x: Counter(x.split()))\n",
    "df_ref_exps_rep['word_pdist'] = df_ref_exps_rep.apply(get_pdist, axis=1)\n",
    "df_ref_exps_rep['word_pdist_numeric'] = df_ref_exps_rep['word_pdist'].apply(lambda dist: list(dist.values()))\n",
    "df_ref_exps_rep.index=df_ref_exps_rep['rep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# calculate difference in proportion between reps\n",
    "\n",
    "rep_a = 1\n",
    "rep_b = 4\n",
    "\n",
    "rep_diff = {}\n",
    "\n",
    "for _, (k, rep_a_v) in enumerate(df_ref_exps_rep.loc[rep_a,'word_pdist'].items()):\n",
    "    rep_diff[k] = df_ref_exps_rep.loc[rep_b,'word_pdist'][k] - rep_a_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# find largest n increase/ decrease in proportion across reps\n",
    "n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433eff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# find the largest increase in proportion between reps\n",
    "top_n = dict(sorted(rep_diff.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfae0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "# find the largest decrease in proportion between reps\n",
    "\n",
    "bottom_n = dict(sorted(rep_diff.items(), key=lambda item: item[1], reverse=False)[:n])\n",
    "\n",
    "bottom_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca172940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "df_grouped = df_ref_exps.groupby('rep').agg({'BOW_lemmatized': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15370ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MASKED FROM STUDENT VERSION\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "font = {'fontname':'Helvetica'}\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "x_limit = 6\n",
    "\n",
    "labels, values = zip(*rep_diff.items())\n",
    "\n",
    "# sort your values in descending order\n",
    "indSort_high = np.argsort(values)[::-1]\n",
    "indSort_low = np.argsort(values)\n",
    "\n",
    "# rearrange your data\n",
    "#labels = np.array(labels)[indSort_high][:x_limit][::-1]\n",
    "labels = np.concatenate([np.array(labels)[indSort_low][:x_limit],np.array(labels)[indSort_high][:x_limit][::-1]])\n",
    "#values = np.array(values)[indSort_high][:x_limit][::-1]\n",
    "values = np.concatenate([np.array(values)[indSort_low][:x_limit], np.array(values)[indSort_high][:x_limit][::-1]])\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(7, 11), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(indexes, values, color = \"#7D7D7D\")\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "# add labels\n",
    "plt.yticks(fontsize=16, **font)\n",
    "plt.xticks(indexes + bar_width, labels,  rotation='vertical', fontsize=16, **font)\n",
    "plt.ylabel(\"change in proportion\", size = 24, **font)\n",
    "plt.yticks(np.arange(-.13,.06, .02))\n",
    "ax.axes.get_xaxis().set_visible(True)\n",
    "#plt.title(\"highest delta words\", size = 24, **font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236c851",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've seen that participants' language became more concise as they reconstructed the same towers with the same partner. This trend was accompanied by an shift in the abstraction level of referring expressions, from lower-level expressions referring to individual blocks, to higher-level expressions that encompassed entire towers.\n",
    "\n",
    "Well done! You're all done with Notebook 1! Take a break, rest your eyes, have a snack.\n",
    "\n",
    "In the [next notebook](/notebooks/ca_programs.ipynb), we begin to explain this process by modeling the learning of part concepts with program abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f23ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb9121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca_env",
   "language": "python",
   "name": "ca_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
