{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d96f35",
   "metadata": {},
   "source": [
    "# Linguistic Analyses for Compositional Abstractions\n",
    "## Notebook 3: Forming conventions to talk about shared abstractions\n",
    "\n",
    "This notebook was written by Robert Hawkins.  \n",
    "Original modeling by Robert Hawkins, Will McCarthy, Cameron Holdaway, Haoliang Wang, and Judy Fan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299ba5b",
   "metadata": {},
   "source": [
    "### *NOTE: THIS NOTEBOOK SERVES AS THE INSTRUCTOR VERSION*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc403a5",
   "metadata": {},
   "source": [
    "In the previous notebook we studied a **concept learning** problem. We inferred libraries of program fragments corresponding to a collection of possible concepts people may have in their heads as they advance through the task. For each trial, we then generated a set of programs expressing the scene in different ways using different abstractions. \n",
    "\n",
    "In this notebook, we extend our model to address the **communication** problem. That is, given a tower scene and a set of concepts in the Architect's head, what linguistic instructions should they give to the Builder that will allow them to successfully reconstruct that scene? \n",
    "\n",
    "We approach this problem in a **probabilistic modeling** framework that extends the model of convention formation described by [Hawkins et al. (2023)](https://cocosci.princeton.edu/papers/hawkinspartners.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1995be1",
   "metadata": {},
   "source": [
    "This notebook is divided into 3 sections that incrementally build up to the full model.\n",
    "\n",
    "**Section 1** begins by implementing the core notion of a *mental lexicon* -- a mapping from words to concepts.\n",
    "\n",
    "**Section 2** implements a basic Architect and Builder agent that make decisions based on fixed lexicons.  \n",
    "\n",
    "**Section 3** equips these agents with the ability to *learn* and update their *beliefs* about the lexicon over time.\n",
    "\n",
    "**Section 4** finally reaches the core theoretical question of why speakers prefer one level of abstraction over another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf1df7-e955-4a46-8be7-bd72d30c652f",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3fa8c9-e4bb-4963-a6d4-96fa3aeec6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aadc094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classes for our model\n",
    "sys.path.append(\"../model/convention_formation/\")\n",
    "from distribution import *\n",
    "from lexicon import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9d279",
   "metadata": {},
   "source": [
    "## Section 1: Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8da56-7f7c-494e-8b0a-fe9ca33a7d65",
   "metadata": {},
   "source": [
    "### Representing lexicons\n",
    "\n",
    "The basic building block of our model is an agent's **mental lexicon**, a particular set of correspondances between concepts and words. To define an agent model, we must first define the mental lexicon. \n",
    "\n",
    "We have created a class for the purpose called `BlockLexicon()` which manages the mapping between program primitives (as defined in **[the previous section](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/notebooks/ca_programs.ipynb)**) to words and phrases. If you're interested in digging into the nitty-gritty details, we've put this class in a helper library __[here](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/model/convention_formation/lexicon.py)__).\n",
    "\n",
    "For our purposes here, though, we don't need to know exactly what's going on under the hood. Let's take the lexicon class out for a drive. We need to initialize it with two parameters, essentially corresponding to the base entities on each side of the mapping we want to define.\n",
    "\n",
    "(1) `dsl`: a list of concepts in the DSL that might be expressed as words,\n",
    "\n",
    "(2) `lexemes`: a list of words that are available to bind to new concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa3338-38b8-46de-81e2-d5b1b85a405e",
   "metadata": {},
   "source": [
    "To build up our intuitions, we're going to work with a very simplified example lexicon, where the set of concepts is the library on trial 10 of the task (retrieved from the file we saved out in the previous section) and the list of lexemes is just a bunch of nonsense words that won't start with any meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84148612",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ../data/model/dsls/2/programs_for_you/programs_ppt_1.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pull out the DSL primitives accessible at trial 10\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/model/dsls/2/programs_for_you/programs_ppt_1.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dsl \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdsl\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define a set of meaningless placeholder words available to be bound to meanings\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/json/_json.py:733\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    731\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/json/_json.py:818\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlines:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows can only be passed if lines=True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 818\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/json/_json.py:874\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    866\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    873\u001b[0m ):\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filepath_or_buffer\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ../data/model/dsls/2/programs_for_you/programs_ppt_1.json does not exist"
     ]
    }
   ],
   "source": [
    "# Pull out the DSL primitives accessible at trial 10\n",
    "d = pd.read_json('../data/model/dsls/2/programs_for_you/programs_ppt_1.json')\n",
    "dsl = d['dsl'][10]\n",
    "\n",
    "# Define a set of meaningless placeholder words available to be bound to meanings\n",
    "lexemes = ['blah', 'blab', 'bloop', 'bleep', 'floop'] \n",
    "l = BlockLexicon(dsl, lexemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b3226-fcb3-4cd6-bcfc-c8ca77df9f93",
   "metadata": {},
   "source": [
    "This lexicon object `l` that we've created has a few basic functions we can call. We can look up the language for any element of the `dsl`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb79b25-0cb4-42fd-bac1-067bc15ea9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dsl[0], '->', l.dsl_to_language(dsl[0]))\n",
    "print(dsl[10], '->', l.dsl_to_language(dsl[10]))\n",
    "print(dsl[-1], '->', l.dsl_to_language(dsl[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c86e7-880f-4736-8029-1d32d9247fc0",
   "metadata": {},
   "source": [
    "and we can also go in the other direction, converting from a linguistic expression to a corresponding primitive in the DSL.\n",
    "We went ahead and 'baked in' correspondences for the basic elements of our DSL, because we're assuming these meanings are pretty much deterministic; there's not much wiggle room about what 'place a horizontal block' or 'move to the left by 8' corresponds to. \n",
    "\n",
    "> sidenote: if we wanted a more realistic model that worked on generic natural language, e.g. if we wanted to pair our agent with a real user writing in a chat box, we would want something more robust that doesn't require exact string matching. We might use a simple algorithm to find the nearest string in the lexicon, or we might use a fancier model operating over utterance embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de665913-c211-4c9d-b001-09f2394c5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('place a horizontal block. ->', l.language_to_dsl('place a horizontal block.'))\n",
    "print('move to the left by 8 ->', l.language_to_dsl('move to the left by 8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af7db7-2e4f-4bfc-809c-a2eb8129c26e",
   "metadata": {},
   "source": [
    "There's one more interesting feature to notice about our `BlockLexicon()` class. If we pass in an unfamiliar utterance that isn't found in the list of lexemes we provided, it will return one of the concepts that doesn't already have a word assigned (and vice versa). We can think of this like the agent randomly 'guessing' rather than erroring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b17b2-a3f1-43df-a90d-5b16dc9d36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('place a blah. ->', l.language_to_dsl('place a blah.'))\n",
    "print('place a flomp. ->', l.language_to_dsl('place a womp.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af5810-abbb-430e-aa96-62edd23e0b9b",
   "metadata": {},
   "source": [
    "### Representing beliefs about lexicons\n",
    "\n",
    "Our lexicon `l` so far has been a deterministic data structure; it's just a single mapping. \n",
    "\n",
    "In a probabilistic model, however, we want to be able to talk in a mathematically rigorous way about an agent's subjective **beliefs**. In other words, we want to be able to define a **probability distribution** over possible lexicons. This will provide a precise definition for an agent's **uncertainty** about the lexicon in their partner's head, i.e. they assume their partner is using some mapping `l*`, but do not know ahead of time exactly what it is.\n",
    "\n",
    "For this example, we're going to construct a distribution as an object that maintains the probabilities for each possible lexicon. (Again, for nitty-gritty details, see the helper library __[here](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/model/convention_formation/distribution.py)__). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca80cec-d3f8-4bad-b376-99e3765721b7",
   "metadata": {},
   "source": [
    "We'll define a **prior** distribution (the agent's initial beliefs) as a uniform distribution over all possible ways of binding elements in the DSL to the lexemes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9355dc77-4b9f-478c-a41d-b55db4e16682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dsl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# [f(x) for x in collection] is called a list comprehension \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# it creates a new list by applying some transformation to each element in a collection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# here we're relying on the fact that `BlockLexicon` uses the *order* of the list of lexemes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m prior \u001b[38;5;241m=\u001b[39m UniformDistribution(\n\u001b[0;32m----> 5\u001b[0m     [BlockLexicon(dsl, \u001b[38;5;28mlist\u001b[39m(mapping)) \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mpermutations(lexemes)]\n\u001b[1;32m      6\u001b[0m )\n",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# [f(x) for x in collection] is called a list comprehension \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# it creates a new list by applying some transformation to each element in a collection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# here we're relying on the fact that `BlockLexicon` uses the *order* of the list of lexemes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m prior \u001b[38;5;241m=\u001b[39m UniformDistribution(\n\u001b[0;32m----> 5\u001b[0m     [BlockLexicon(\u001b[43mdsl\u001b[49m, \u001b[38;5;28mlist\u001b[39m(mapping)) \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mpermutations(lexemes)]\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dsl' is not defined"
     ]
    }
   ],
   "source": [
    "# [f(x) for x in collection] is called a list comprehension \n",
    "# it creates a new list by applying some transformation to each element in a collection\n",
    "# here we're relying on the fact that `BlockLexicon` uses the *order* of the list of lexemes\n",
    "prior = UniformDistribution(\n",
    "    [BlockLexicon(dsl, list(mapping)) for mapping in itertools.permutations(lexemes)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011da703",
   "metadata": {},
   "source": [
    "We've also created some handy helper functions on distributions. For example, we can 'marginalize' to look at the possible mappings for any single chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d432c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example lexicon')\n",
    "print('possible values of chunk_L : ', \n",
    "      json.dumps(prior.marginalize(lambda d : d['chunk_L']), indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3f57f",
   "metadata": {},
   "source": [
    "Here, we can see that 'chunk_L' in the DSL is initially equally likely to be mapped to any utterance: $P([[u]] = L) = 1/|U|$ The equal probabilities for each expression tell us that the Agents think each expression is an equally good (or bad) translation of \"chunk_L\". This makes sense for this artificial langauge (where \"bleeps\" and \"bloops\" are used to refer to abstractions). Real people likely have strong priors about what words will mean (we can make quite a lot of sense of \"build an L\" before any shared experience, even if there's some remaining uncertainty about properties of the L like its size and width, etc). But a uniform prior helps us understand the dynamics of coordination in the most extreme case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cf2ea-0435-45a7-8a42-5fe3354fd047",
   "metadata": {},
   "source": [
    "Here are a few other things you can do with a distribution object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5fc4c4a-97db-446e-9b84-7803430a287c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prior' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call d.support() to get the support, i.e. the list of lexicon values that it is defined over\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# e.g. we can print out the first lexicon as an example\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample element:\u001b[39m\u001b[38;5;124m'\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(\u001b[43mprior\u001b[49m\u001b[38;5;241m.\u001b[39msupport()[\u001b[38;5;241m0\u001b[39m], indent \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP(lexicon) =\u001b[39m\u001b[38;5;124m'\u001b[39m, prior\u001b[38;5;241m.\u001b[39mscore(prior\u001b[38;5;241m.\u001b[39msupport()[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prior' is not defined"
     ]
    }
   ],
   "source": [
    "# d.support() returns the support of the distribution d, i.e. the list of values that it is defined over\n",
    "# e.g. we can print out the first lexicon in the support as an example of what lexicons look like\n",
    "print('example element:', json.dumps(prior.support()[0], indent = 4))\n",
    "\n",
    "# d.score(val) returns the probability of val in the distribution d\n",
    "# e.g. this is the probability assigned to the lexicon we just printed out\n",
    "print('P(lexicon) =', prior.score(prior.support()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75db145e-b86f-44bc-b91c-da43ec19cbd6",
   "metadata": {},
   "source": [
    "## Section 2: Simulating Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55489f03",
   "metadata": {},
   "source": [
    "Now we have a way of representing an agent's beliefs over lexicons, we can define an Architect and Builder.\n",
    "\n",
    "Both maintain their own belief distribution about the *other agent's* lexicon.\n",
    "\n",
    "The **Architect** agent makes choices about what to say based by imaginging how the Builder will interpret them.  \n",
    "\n",
    "The **Builder** takes actions based on its beliefs about what the Architect would say in different situations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59a39bce-9691-437e-99d0-e53c453c2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedAgent() :\n",
    "    def __init__(self, role, trial) :\n",
    "        '''\n",
    "        Args: \n",
    "           * role: string giving agent's role in the task ('architect' or 'builder')\n",
    "           * trial: dictionary of meta-data about the current trial \n",
    "        '''\n",
    "        self.role = role\n",
    "        self.actions = trial['dsl']\n",
    "\n",
    "        # initialize beliefs to a uniform prior over possible lexicons, as above\n",
    "        self.possible_lexicons = set([BlockLexicon(self.actions, list(mapping)) \n",
    "                                      for mapping in itertools.permutations(lexemes)])\n",
    "        self.beliefs = UniformDistribution(self.possible_lexicons)\n",
    "        self.utterances = set(list(self.possible_lexicons)[0].values())\n",
    "        \n",
    "    def act(self, observation) :\n",
    "        '''\n",
    "        produce an action based on role and current beliefs\n",
    "        '''\n",
    "        if self.role == 'architect' :\n",
    "            # Architect is going to build up a distribution over utterances to say\n",
    "            utt_dist = EmptyDistribution()\n",
    "            for lexicon in self.beliefs.support() :\n",
    "                # They imagine what they would say under each possible lexicon \n",
    "                # and weight it by the likelihood that the builder is actually using that lexicon\n",
    "                utt_dist.update({lexicon.dsl_to_language(observation) : self.beliefs.score(lexicon)})\n",
    "            return choice(a = [*utt_dist.support()], \n",
    "                          p = [utt_dist.score(u) for u in utt_dist.support()])\n",
    "\n",
    "        if self.role == 'builder' :\n",
    "            # get P(a | utt) by marginalizing over lexicons \n",
    "            action_dist = EmptyDistribution()\n",
    "            for lexicon in self.beliefs.support() :\n",
    "                action_dist.update({lexicon.language_to_dsl(observation) : self.beliefs.score(lexicon)})\n",
    "            return choice(a = [*action_dist.support()], \n",
    "                          p = [action_dist.score(a) for a in action_dist.support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5b701-352f-4397-8ae1-cc61ba7fab7f",
   "metadata": {},
   "source": [
    "Because our focus is on modeling abstractions learned during the task, we assume that Architect Agents and Builder Agents can unambiguously communicate about the base DSL-- moving left and right and placing individual blocks. I.e. when an Architect wants a Builder to place a block they will always say \"place a horizontal block\", and the Builder will correctly interpret this utterance.\n",
    "\n",
    "The only thing that varies across different lexicons is the words used for *learned* program fragments. In practice (for this example) only five of these were learned across all participants' trial sequences. The set of possible lexicons is therefore fully defined by the set of possible mappings from these fragments to the `lexemes` defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e777d9b9-1e58-4cd9-9dbf-90be88d60d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "architect = FixedAgent('architect', d.loc[0].to_dict())\n",
    "print('architect choice: ', architect.act('h'))\n",
    "\n",
    "builder = FixedAgent('builder', d.loc[0].to_dict())\n",
    "print('builder choice: ', builder.act('place a horizontal block.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352243eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "architect = FixedAgent('architect', d.loc[0].to_dict())\n",
    "print('architect choice: ', architect.act('chunk_L'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020e990-db06-4ed1-ac7a-5f153c0c91b1",
   "metadata": {},
   "source": [
    "### Running simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b0db7-b394-4b59-ba16-c3177bc42ced",
   "metadata": {},
   "source": [
    "Now we have our agents, we need to run them forward through the trial sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ff4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.04 # weight that trades off between TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cafdc-5a6f-451f-a557-ea63c80dd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation() :\n",
    "    output = pd.DataFrame({\"utt\": [], \"response\": [], \"target_program\": [], \"target_length\" : [], \"acc\": []})\n",
    "    for i, trial in d.iterrows() :\n",
    "        architect = FixedAgent('architect', trial)\n",
    "        builder = FixedAgent('builder', trial)\n",
    "\n",
    "        # architect selects which program representation to comunicate proportional to length\n",
    "        possiblePrograms = list(trial['programs_with_length'].keys())\n",
    "        possibleLengths = np.array(list(trial['programs_with_length'].values()))\n",
    "        utilities = np.exp(-alpha * possibleLengths) / sum(np.exp(-alpha * possibleLengths))\n",
    "        target_program = choice(a = possiblePrograms, p = utilities)\n",
    "\n",
    "        # loop through steps of target program one at a time\n",
    "        utts, responses, accs = [], [], []\n",
    "        for step in target_program.split(' ') :\n",
    "            utt = architect.act(step)\n",
    "            response = builder.act(utt)\n",
    "            utts.append(utt)\n",
    "            responses.append(response)\n",
    "            accs.append(1.0 * (response == step))\n",
    "\n",
    "        output = pd.concat([output, pd.DataFrame({\n",
    "            \"trial\": int(i),\n",
    "            \"utt\": utts,\n",
    "            \"response\": responses,\n",
    "            \"acc\": accs,\n",
    "            \"target_program\": target_program,\n",
    "            \"target_length\" : trial['programs_with_length'][target_program],\n",
    "        })])\n",
    "    return output\n",
    "\n",
    "run_0 = run_simulation()\n",
    "display(run_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4699543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the first trial\n",
    "print('target program: \\n', run_0.query('trial==0').loc[0,'target_program'])\n",
    "run_0.query('trial==0')[['utt','response','acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbf043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the final trial\n",
    "print('target program: \\n', run_0.query('trial==11').loc[0,'target_program'])\n",
    "\n",
    "run_0.query('trial==11')[['utt','response','acc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6692312-fff3-4621-83b0-393c1f70268a",
   "metadata": {},
   "source": [
    "### <span style=\"color: orange\"> Exercise: explore how accuracy changes </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a12ad-eac1-4c94-a115-2f10c42505f7",
   "metadata": {},
   "source": [
    "Wait, why is the accuracy so bad? Well, our agents aren't actually *learning* -- they're continuing to use their initial uniform priors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e771ae8-e64c-409d-8670-bfca342c043b",
   "metadata": {},
   "source": [
    "## Section 3: Simulating learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe82f26-4c60-451a-a2d5-4ab4f6fb9118",
   "metadata": {},
   "source": [
    "To have our agents learn, we need to extend the agent class to do Bayesian inference.\n",
    "\n",
    "Here we add an update_beliefs function, which performs a Bayesian update on the beliefs about the other agent's lexicon based on the outcome of the previous trial. Note that this update happens when the class is initialized, so really we're defining a new agent at each step with updated beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb28cf-8efd-4852-9110-998011ba7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningAgent(FixedAgent) :\n",
    "    def __init__(self, role, curr_trial, previous_trial_df) :\n",
    "        super().__init__(role, curr_trial)\n",
    "        combined_primitives = set().union(*previous_trial_df['dsl']) if not previous_trial_df.empty else self.actions\n",
    "        self.possible_lexicons = set([BlockLexicon(set().union(combined_primitives), list(mapping)) \n",
    "                                      for mapping in itertools.permutations(lexemes)])\n",
    "        self.utterances = set(list(self.possible_lexicons)[0].values())\n",
    "        self.update_beliefs(previous_trial_df)\n",
    "\n",
    "    def update_beliefs(self, previous_trial_df) :\n",
    "        # Initialize posterior \n",
    "        posterior = EmptyDistribution()\n",
    "        posterior.to_logspace()\n",
    "\n",
    "        # for each data point, calculate the marginal likelihood under lexicon distribution\n",
    "        # P(l | obs) = 1/Z * P(l) * \\prod_{o \\in obs} P(o | l)\n",
    "        # log P(l|obs) = -log Z + log P(l) + \\sum_{o \\in obs} log P(o | l)\n",
    "        for lexicon in self.beliefs.support() :\n",
    "            prior_term = np.log(self.beliefs.score(lexicon))\n",
    "            likelihood_term = 0\n",
    "            for i, step in previous_trial_df.iterrows() :\n",
    "                if self.role == 'builder' :\n",
    "                    likelihood_term += np.log(self.A1(step.target, lexicon).score(step.utterance))\n",
    "                elif self.role == 'architect' :\n",
    "                    likelihood_term += np.log(self.B0(step.utterance, lexicon).score(step.response))\n",
    "            posterior.update({lexicon : prior_term + likelihood_term})\n",
    "        posterior.renormalize()\n",
    "        posterior.from_logspace()\n",
    "        self.beliefs = posterior\n",
    "        \n",
    "    def B0(self, utt, lexicon) :\n",
    "        builder_dist = EmptyDistribution()\n",
    "        for action in self.actions :\n",
    "            builder_dist.update({action : 1 if action == lexicon.language_to_dsl(utt) else 0.01})\n",
    "        builder_dist.renormalize()\n",
    "        return builder_dist\n",
    "        \n",
    "    def A1(self, target, lexicon) :\n",
    "        architect_dist = EmptyDistribution()\n",
    "        for utt in self.utterances :\n",
    "            architect_dist.update({utt : 1 if utt == lexicon.dsl_to_language(target) else 0.01})\n",
    "        architect_dist.renormalize()\n",
    "        return architect_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2c439-4adc-4194-a223-6bab282f3410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_learning_simulation(verbose = False) :\n",
    "    output = pd.DataFrame({\"utterance\": [], \"response\": [], \"target\": [], \"full_program\" : [], \"target_length\" : [], \"dsl\" : [], \"acc\": []})\n",
    "    for i, trial in d.iterrows() :\n",
    "        architect = LearningAgent('architect', trial, output) # create agent with updated beliefs\n",
    "        builder = LearningAgent('builder', trial, output)     # create agent with updated beliefs\n",
    "        \n",
    "        # architect selects which program representation to comunicate proportional to length\n",
    "        possiblePrograms = list(trial['programs_with_length'].keys())\n",
    "        possibleLengths = np.array(list(trial['programs_with_length'].values()))\n",
    "        utilities = np.exp(-alpha * possibleLengths) / sum(np.exp(-alpha * possibleLengths))\n",
    "        target_program = choice(a = possiblePrograms, p = utilities)\n",
    "\n",
    "        # loop through steps of target program one at a time\n",
    "        target_steps, utts, responses, accs = [], [], [], []\n",
    "        for step in target_program.split(' ') :\n",
    "            utt = architect.act(step)\n",
    "            response = builder.act(utt)\n",
    "            target_steps.append(step)\n",
    "            utts.append(utt)\n",
    "            responses.append(response)\n",
    "            accs.append(response == step)\n",
    "\n",
    "        if verbose:\n",
    "            print('trial', i)\n",
    "            print(pd.DataFrame({'utts' : utts, 'responses' : responses, 'correct' : accs, 'target' : target_steps}))\n",
    "            print('beliefs about chunk_C meaning', \n",
    "                  json.dumps(architect.beliefs.marginalize(lambda d : d['chunk_C'] if 'chunk_C' in d else None), indent = 4))\n",
    "        \n",
    "        output = pd.concat([output, pd.DataFrame({\n",
    "            \"trial\": i,\n",
    "            \"utterance\": utts,\n",
    "            \"response\": responses,\n",
    "            \"acc\": accs,\n",
    "            \"target\" : target_steps,\n",
    "            \"full_program\": target_program,\n",
    "            \"dsl\" : [trial['dsl']] * len(utts),\n",
    "            \"target_length\" : trial['programs_with_length'][target_program],\n",
    "        })])\n",
    "    return output\n",
    "\n",
    "learning_run_0 = run_learning_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_run_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78473d56-4367-4db1-9445-5de7e7dc6b13",
   "metadata": {},
   "source": [
    "## Section 4: Choosing programs.\n",
    "\n",
    "In principle, words allow us to communicate arbitrarily complex concepts, and our mental representation of linguistic meaning is flexible enough to be updated over time. To the extent that both Architects and Builders are learning to 'chunk' blocks over time, each  program fragment) could, in principle, be assigned a new word or phrase, and conveyed directly through the Architects' instructions. Well, almost. There is no guarantee that the words the Architect chooses to pick out a new concept would invoke the same concept in someone else. There is actually uncertainty about how the Builder will interpret the Architects' instructions and this, we suggest, might change what the Architect chooses to say.\n",
    "\n",
    "Our hypothesis is that Architects trade-off communicative *efficiency* with communicative *effectiveness*. While they generally want to say things concisely, if there is too much uncertainty about how the Builder will interpret their words, they will choose a less ambiguous (if more wordy) way of expressing the same information. Concretely, if the Architect wanted to say \"build an L\" but they thought that the Builder wouldn't get what \"L\" meant, they might spell out the steps to make an L-shaped tower instead.\n",
    "\n",
    "Wow, this is great! We can see our agents are updating their beliefs about the lexicon over time and able to get somewhat more accurate as they coordinate. But one of the most interesting things about our empirical data is that speakers seem to be strategically choosing which representation of the tower to convey -- our best current theory of why participants reduce the length of their utterances over time is that even when new library chunks come online, architects don't always try to refer to them right away. They aren't confident enough that their partner will understand, as the block-level descriptions are much safer. However, the block-level descriptions are also much *costlier* in terms of time and effect because they have to laboriously describe one action at a time. \n",
    "\n",
    "So far, we just used a placeholder for how the speaker picks which representation to communication: they just randomly pick from the list of candidates, slightly preferring shorter programs. However, there are other considerations that ought to go into this decision, namely the estimated likelihood that the listener will do the right thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d524d4-92bf-4013-a842-bed42df01678",
   "metadata": {},
   "source": [
    "As an exercise, add one line of code to the simulation to weight the target program according to its utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c1b7c8-7e3b-4732-b4b1-12baa57c82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_strategic_simulation() :\n",
    "    output = pd.DataFrame({\"utterance\": [], \"response\": [], \"target\": [], \"full_program\" : [], \"target_length\" : [], \"dsl\" : [], \"acc\": []})\n",
    "    for i, trial in d.iterrows() :\n",
    "        architect = LearningAgent('architect', trial, output)\n",
    "        builder = LearningAgent('builder', trial, output)\n",
    "        \n",
    "        # architect selects which program representation to comunicate proportional to length\n",
    "        possiblePrograms = list(trial['programs_with_length'].keys())\n",
    "        possibleLengths = np.array(list(trial['programs_with_length'].values()))\n",
    "        \n",
    "        ## \n",
    "        ## utilities = np.exp(-alpha * possibleLengths) / sum(np.exp(-alpha * possibleLengths))\n",
    "        ## \n",
    "        \n",
    "        target_program = choice(a = possiblePrograms, p = utilities)\n",
    "\n",
    "        # loop through steps of target program one at a time\n",
    "        target_steps, utts, responses, accs = [], [], [], []\n",
    "        for step in target_program.split(' ') :\n",
    "            utt = architect.act(step)\n",
    "            response = builder.act(utt)\n",
    "            target_steps.append(step)\n",
    "            utts.append(utt)\n",
    "            responses.append(response)\n",
    "            accs.append(response == step)\n",
    "\n",
    "        print('trial', i)\n",
    "        print(pd.DataFrame({'utts' : utts, 'responses' : responses, 'correct' : accs, 'target' : target_steps}))\n",
    "        print('beliefs about chunk_C meaning', \n",
    "              json.dumps(architect.beliefs.marginalize(lambda d : d['chunk_C'] if 'chunk_C' in d else None), indent = 4))\n",
    "        output = pd.concat([output, pd.DataFrame({\n",
    "            \"utterance\": utts,\n",
    "            \"response\": responses,\n",
    "            \"acc\": accs,\n",
    "            \"target\" : target_steps,\n",
    "            \"full_program\": target_program,\n",
    "            \"dsl\" : [trial['dsl']] * len(utts),\n",
    "            \"target_length\" : trial['programs_with_length'][target_program],\n",
    "        })])\n",
    "    return output\n",
    "display(run_learning_simulation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ad286",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "\n",
    "Congratulations! You've finished the tutorial!\n",
    "\n",
    "We hope you found it fun and informative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
