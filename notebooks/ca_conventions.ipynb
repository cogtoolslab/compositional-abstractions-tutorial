{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60eda1d3",
   "metadata": {},
   "source": [
    "# Linguistic Analyses for Compositional Abstractions\n",
    "## Notebook 3: Forming conventions to talk about shared abstractions\n",
    "\n",
    "Preparation of this notebook was led by [Robert Hawkins](https://rdhawkins.com/).\n",
    "\n",
    "These results were originally reported in: \n",
    "[McCarthy*, W., Hawkins*, R., Wang, H., Holdaway, C., and Fan, J. (2021). Learning to communicate about shared procedural abstractions. Proceedings of the 43rd Annual Meeting of the Cognitive Science Society.](https://cogtoolslab.github.io/pdf/mccarthy_cogsci_2021b.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586be7a1",
   "metadata": {},
   "source": [
    "### *NOTE: THIS NOTEBOOK SERVES AS THE INSTRUCTOR VERSION*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70dbc96",
   "metadata": {},
   "source": [
    "In the previous notebook we studied a **concept learning** problem. We inferred libraries of program fragments corresponding to a collection of possible concepts people may have in their heads as they advance through the task. For each trial, we then generated a set of programs expressing the scene in different ways using different abstractions. \n",
    "\n",
    "In this notebook, we extend our model to address the **communication** problem. That is, given a tower scene and a set of concepts in the Architect's head, what linguistic instructions should they give to the Builder that will allow them to successfully reconstruct that scene? \n",
    "\n",
    "We approach this problem in a **probabilistic modeling** framework that extends the model of convention formation described by [Hawkins et al. (2023)](https://cocosci.princeton.edu/papers/hawkinspartners.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5706815",
   "metadata": {},
   "source": [
    "This notebook is divided into 4 sections that incrementally build up to the full model.\n",
    "\n",
    "**Section 1** begins by implementing the core notion of a *mental lexicon* -- a mapping from words to concepts.\n",
    "\n",
    "**Section 2** implements basic Architect and Builder agents that make decisions based on fixed lexicons.  \n",
    "\n",
    "**Section 3** equips these agents with the ability to *learn* and update their *beliefs* about the lexicon over time.\n",
    "\n",
    "**Section 4** finally reaches the core theoretical question of why speakers prefer one level of abstraction over another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52653c",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6113e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classes for our model\n",
    "sys.path.append(\"../model/convention_formation/\")\n",
    "from distribution import *\n",
    "from lexicon import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b3aa3",
   "metadata": {},
   "source": [
    "## Section 1: Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51826c3",
   "metadata": {},
   "source": [
    "### Representing lexicons\n",
    "\n",
    "The basic building block of our model is an agent's **mental lexicon**, a particular set of correspondances between concepts and words. To define an agent model, we must first define the mental lexicon. \n",
    "\n",
    "We have created a class for this purpose called `BlockLexicon()` which manages the mapping between program primitives (as defined in **[the previous section](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/notebooks/ca_programs.ipynb)**) to words and phrases. If you're interested in digging into the nitty-gritty details, we've put this class in a helper library __[here](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/model/convention_formation/lexicon.py)__).\n",
    "\n",
    "For our purposes, though, we don't need to know exactly what's going on under the hood. Let's take the lexicon class out for a drive. We need to initialize it with two parameters, essentially corresponding to the base entities on each side of the mapping we want to define.\n",
    "\n",
    "(1) `dsl`: a list of concepts in the domain-specific language (DSL) that might be expressed as words\n",
    "\n",
    "(2) `lexemes`: a list of words that are available to bind to new concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fecd957",
   "metadata": {},
   "source": [
    "To build up our intuitions, we're going to work with a very simplified example lexicon, where the set of concepts is the library on trial 10 of the task (retrieved from the file we saved out in the previous section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d562b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the DSL primitives accessible at trial 10\n",
    "empirical_data = pd.read_json('../data/model/programs_for_you/programs_ppt_1.json')\n",
    "dsl = empirical_data.iloc[10]['dsl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fea7eb",
   "metadata": {},
   "source": [
    "We'll also define a list of lexemes containing just a bunch of nonsense words that won't start out meaning anything. And then we will construct a `BlockLexicon` (defined [here](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/model/convention_formation/lexicon.py)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of meaningless placeholder words available to be bound to meanings\n",
    "lexemes = ['blah', 'blab', 'bloop', 'bleep', 'floop'] \n",
    "l = BlockLexicon(dsl, lexemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4bf9e",
   "metadata": {},
   "source": [
    "This lexicon object `l` comes with a lot of built-in properties, including how to talk about placing individual blocks. We can check these out by calling some of its functions. For example, the `dsl_to_language` method looks up which word to use for any element of the `dsl`. \n",
    "\n",
    "> _Note_: We use the term \"method\" to refer to a function that belongs to a Python class (e.g., `dsl_to_language` is a method of the `BlockLexicon` class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first element in the DSL (h) is already bound to the language \"place a horizontal block.\"')\n",
    "print(dsl[0], '->', l.dsl_to_language(dsl[0]))\n",
    "print('')\n",
    "print('Here are some more examples.')\n",
    "print(dsl[10], '->', l.dsl_to_language(dsl[10]))\n",
    "print(dsl[-1], '->', l.dsl_to_language(dsl[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5a991",
   "metadata": {},
   "source": [
    "We can also go in the other direction using the `language_to_dsl` function, converting from a linguistic expression to a corresponding primitive in the DSL.\n",
    "We went ahead and 'baked in' correspondences for the basic elements of our DSL, because we're assuming these meanings are pretty much deterministic; there's not much wiggle room about what 'place a horizontal block' or 'move to the left by 8' means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b4cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('place a horizontal block. ->', l.language_to_dsl('place a horizontal block.'))\n",
    "print('move to the left by 8 ->', l.language_to_dsl('move to the left by 8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b162f",
   "metadata": {},
   "source": [
    "> _Note_: If we wanted a more realistic model that worked on generic natural language, e.g. if we wanted to pair our agent with a real user writing in a chat box, we would want something more robust that doesn't require exact string matching. We might use a simple algorithm to find the nearest string in the lexicon, or we might use a fancier model operating over utterance embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77087e",
   "metadata": {},
   "source": [
    "If you play around with the `BlockLexicon()` class, you may notice a subtlety. If we pass in an unfamiliar utterance that isn't found in the list of lexemes we provided (e.g., `place a blah` shown below), it will return one of the concepts that doesn't already have a word assigned. We can think of this like an agent randomly 'guessing' rather than throwing an error. Likewise, if we pass in a concept that is not in the DSL, it will return a randomly sampled element from the list of lexemes. (If you re-run the cell below multiple times, you'll notice that the output will be different each time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Passing in \"place a blah\"...')\n",
    "print('place a blah. ->', l.language_to_dsl('place a blah.'))\n",
    "print('')\n",
    "print('Passing in \"chunk_R\"...')\n",
    "print('chunk_R ->', l.dsl_to_language('chunk_R'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b51509",
   "metadata": {},
   "source": [
    "### Representing beliefs about lexicons\n",
    "\n",
    "The lexicon `l` we've been playing with so far is a deterministic data structure; it's just a single mapping. \n",
    "\n",
    "In a probabilistic model, however, we want to be able to talk in a mathematically rigorous way about an agent's subjective **beliefs** about a mapping. In other words, we want to be able to define a **probability distribution** over all possible lexicons $\\mathcal{L}$. This will provide a precise definition for an agent's uncertainty about the lexicon in their partner's head, i.e. they assume their partner must be using one of these mappings $\\mathcal{L}^*$, but do not know ahead of time exactly what it is.\n",
    "\n",
    "We're going to introduce another custom class we've written called a `Distribution()`, which maintains the probabilities assigned to each possible lexicon. (Again, for nitty-gritty details, see the helper library __[here](https://github.com/cogtoolslab/compositional_abstractions_tutorial/blob/main/model/convention_formation/distribution.py)__). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61dd25b",
   "metadata": {},
   "source": [
    "We'll define a **prior** distribution (the agent's initial beliefs) as a uniform distribution over all possible ways of binding elements in the DSL to the lexemes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255d558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initializing a LexiconPrior from a specific DSL and a list of lexemes\n",
    "prior = LexiconPrior(dsl, lexemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e12802",
   "metadata": {},
   "source": [
    "We've also created some handy helper functions to work with distributions. For example, `prior.support()` returns the support of the distribution `prior`, i.e. the list of values that it is defined over. So we can print out the first lexicon in the support as an example of what lexicons look like. In this case, there are 24 lexicons in the support of the prior because there are exactly 24 ways of binding the four non-primitives in the DSL (e.g., \"chunk_L\") to unique lexemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('example element:', prior.support()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4263d",
   "metadata": {},
   "source": [
    "We can also use `prior.score(l)` to return the probability of a lexicon `l` under the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa02ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prior.support()))\n",
    "print('P(^ that lexicon) = 1/24 = ', prior.score(prior.support()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7235c0b",
   "metadata": {},
   "source": [
    "Here are a few other things you can do with a distribution object. For example, we can use the `marginalize` function to look at the possible mappings for any single chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef194efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('possible utterances for chunk_L : ', \n",
    "      prior.marginalize(lambda l : l.dsl_to_language('chunk_L')))\n",
    "\n",
    "print('possible concepts for \"place a blah\" : ', \n",
    "      prior.marginalize(lambda l : l.language_to_dsl('place a blah.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba7b9f",
   "metadata": {},
   "source": [
    "Here, we can verify that 'chunk_L' in the DSL is initially equally likely to be mapped to any utterance: $p = 1/|U|$ The equal probabilities for each expression tell us that the Agents think each expression is an equally good (or bad) translation of \"chunk_L\".\n",
    "\n",
    "> _Note:_  This makes sense for this artificial language (where \"bleeps\" and \"bloops\" are used to refer to abstractions). Real people likely have strong priors about what words will mean (we can make quite a lot of sense of \"build an L\" before any shared experience, even if there's some remaining uncertainty about properties of the L like its size and width, etc). But a uniform prior helps us understand the dynamics of coordination in the most extreme case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e2740",
   "metadata": {},
   "source": [
    "## Section 2: Simulating Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f96f59",
   "metadata": {},
   "source": [
    "Now we have a way of representing an agent's beliefs over lexicons, we can define an Architect and Builder.\n",
    "\n",
    "Both maintain their own belief distribution about the *other agent's* lexicon.\n",
    "\n",
    "The **Architect** agent makes choices about what to say based by imaginging how the Builder will interpret them.  \n",
    "\n",
    "The **Builder** takes actions based on its beliefs about what the Architect would say in different situations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedAgent() :\n",
    "    def __init__(self, role, trial) :\n",
    "        '''\n",
    "        Args: \n",
    "           * role: string giving agent's role in the task ('architect' or 'builder')\n",
    "           * trial: dictionary of meta-data about the current trial \n",
    "        '''\n",
    "        self.role = role\n",
    "        self.actions = trial['dsl']\n",
    "\n",
    "        # initialize beliefs to a uniform prior over possible lexicons, as above\n",
    "        self.possible_lexicons = set([BlockLexicon(self.actions, list(mapping)) \n",
    "                                      for mapping in itertools.permutations(lexemes)])\n",
    "        self.beliefs = UniformDistribution(self.possible_lexicons)\n",
    "        self.utterances = list(self.possible_lexicons)[0].utterances\n",
    "        \n",
    "    def act(self, observation) :\n",
    "        '''\n",
    "        produce an action based on role and current beliefs\n",
    "        '''\n",
    "        if self.role == 'architect' :\n",
    "            # Architect is going to build up a distribution over utterances to say\n",
    "            utt_dist = self.beliefs.marginalize(lambda l : l.dsl_to_language(observation))\n",
    "            return utt_dist.sample()\n",
    "\n",
    "        if self.role == 'builder' :\n",
    "            # get P(a | utt) by marginalizing over lexicons \n",
    "            action_dist = self.beliefs.marginalize(lambda l : l.language_to_dsl(observation))\n",
    "            return action_dist.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10713750",
   "metadata": {},
   "source": [
    "Because our focus is on modeling abstractions learned during the task, we assume that Architect Agents and Builder Agents can unambiguously communicate about the base DSL-- moving left and right and placing individual blocks. I.e. when an Architect wants a Builder to place a block they will always say \"place a horizontal block\", and the Builder will correctly interpret this utterance.\n",
    "\n",
    "The only thing that varies across different lexicons is the words used for *learned* program fragments. In practice (for this example) only five of these were learned across all participants' trial sequences. The set of possible lexicons is therefore fully defined by the set of possible mappings from these fragments to the `lexemes` defined above.\n",
    "\n",
    "For the built-in primitives like the horizontal block, this should be a deterministic choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc22497",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_trial = empirical_data.loc[0].to_dict()\n",
    "architect = FixedAgent('architect', first_trial)\n",
    "print('architect utterance: `h` -> ', architect.act('h'))\n",
    "\n",
    "builder = FixedAgent('builder', first_trial)\n",
    "print('builder choice: \"place a horizontal block\" -> ', builder.act('place a horizontal block.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5036bfe",
   "metadata": {},
   "source": [
    "### Running simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0290ad",
   "metadata": {},
   "source": [
    "Now we have our agents, let's see what they do when they proceed through the same sequence of 12 trials as one human dyad from our behavioral experiment.\n",
    "\n",
    "On each trial, the (Fixed) Architect will see a tower scene and then write a program in their DSL that could re-generate the scene. Then for each `step` of the program, we call `architect.act(step)` to get a corresponding linguistic instruction (`utt`, for utterance). We then pass that instruction (`utt`) to the builder agent who will select an action in their own DSL using the function `builder.act(utt)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbce74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(trial_info) :\n",
    "    output = pd.DataFrame({\"utt\": [], \"response\": [], \"target_program\": [], \"target_length\" : [], \"acc\": []})\n",
    "    for i, trial in trial_info.iterrows() :\n",
    "        architect = FixedAgent('architect', trial)\n",
    "        builder = FixedAgent('builder', trial)\n",
    "\n",
    "        # architect randomly selects which of the program representations to comunicate\n",
    "        possiblePrograms = list(trial['programs_with_length'].keys())\n",
    "        target_program = choice(possiblePrograms)\n",
    "\n",
    "        # loop through steps of target program one at a time\n",
    "        utts, responses, accs = [], [], []\n",
    "        for step in target_program.split(' ') :\n",
    "            utt = architect.act(step)\n",
    "            response = builder.act(utt)\n",
    "            utts.append(utt)\n",
    "            responses.append(response)\n",
    "            accs.append(1.0 * (response == step))\n",
    "\n",
    "        output = pd.concat([output, pd.DataFrame({\n",
    "            \"trial\": int(i),\n",
    "            \"utt\": utts,\n",
    "            \"response\": responses,\n",
    "            \"acc\": accs,\n",
    "            \"target_program\": target_program,\n",
    "            \"target_length\" : trial['programs_with_length'][target_program],\n",
    "        })])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3262c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can call the run_simulation function above to simulate the utterances and actions that would be generated \n",
    "# in a 12-trial interaction between the Architect and Builder. Note that if you run it multiple times, you'll get \n",
    "# different simulated outcomes each time. \n",
    "run_0 = run_simulation(empirical_data)\n",
    "display(run_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f645dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the first trial\n",
    "print('target program: \\n', run_0.query('trial==0').loc[0,'target_program'])\n",
    "run_0.query('trial==0')[['utt','response','acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6233e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the final trial\n",
    "print('target program: \\n', run_0.query('trial==11').loc[0,'target_program'])\n",
    "run_0.query('trial==11')[['utt','response','acc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10fa83",
   "metadata": {},
   "source": [
    "Notice that the final trial may use some chunks, but their correspondence to utterances have not changed between the first and final trials. That is because these are \"fixed\" agents who do not yet have a way of updating their lexicons!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc477f6f",
   "metadata": {},
   "source": [
    "### <span style=\"color: orange\"> Exercise: explore how accuracy changes </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c577389",
   "metadata": {},
   "source": [
    "If they do use chunks, you might notice that the accuracy gets worse over time because our agents aren't actually *learning* -- they're continuing to use their initial uniform priors. We can take a look at this by analyzing how accuracy (`acc`) changes across trials in the `run_0` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE BLOCK FOR EXERCISE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a4144",
   "metadata": {},
   "source": [
    "## Section 3: Simulating learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecbdce",
   "metadata": {},
   "source": [
    "In a probabilistic framework, learning is equivalent to updating one's beliefs given new observations. We thus need to extend the `FixedAgent` class (defined in Section 2) with a `update_beliefs()` function, using Bayes' rule to turn their prior into a posterior, $P(\\mathcal{L} | o)$, using the outcomes `o` on the previous trials. Here's Bayes' rule, which gives us a blueprint to handle this:\n",
    "\n",
    "$$P(\\mathcal{L} | o) = \\frac{P(o | \\mathcal{L})P(\\mathcal{L})}{\\sum_{\\mathcal{L}} P(o | \\mathcal{L}) P(\\mathcal{L})}$$\n",
    "\n",
    "We already have the prior term $P(\\mathcal{L})$, but we're missing the likelihood term $P(o | \\mathcal{L})$ which scores how likely a given utterance or action on the previous trials would be under different lexicons $\\mathcal{L}$. \n",
    "\n",
    "Following Hawkins et al. (2023), we use a likelihood where each agent reasons about a (simplified) mental model of the other agent, and asks how likely the other agent would be to make a given choice if they had a given lexicon in their head. Specifically, the architect tries to design utterances to maximize the probability of success assuming a literal builder $B_0(a | u)$, which chooses among actions $a$ that are literally consistent with the utterance $u$ they hear. Meanwhile, the builder tries to pick actions assuming the architect $A_0(u | a^*)$ is choosing utterances $u$ that are literally consistent with the target step $a^*$ they are trying to convey.\n",
    "\n",
    "> _Note_: As an implementational detail, note that we are creating a new agent at each step with the correspondingly updated beliefs based on all previous trials rather than repeatedly updating the same agent trial by trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8887201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningAgent(FixedAgent) :\n",
    "    def __init__(self, role, curr_trial, previous_trial_df) :\n",
    "        super().__init__(role, curr_trial)\n",
    "        combined_primitives = set().union(*previous_trial_df['dsl']) if not previous_trial_df.empty else self.actions\n",
    "        self.possible_lexicons = set([BlockLexicon(set().union(combined_primitives), list(mapping)) \n",
    "                                      for mapping in itertools.permutations(lexemes)])\n",
    "        self.update_beliefs(previous_trial_df)\n",
    "\n",
    "    def B0(self, utt, lexicon) :\n",
    "        '''\n",
    "        simple builder agent that has equal probability\n",
    "        of building anything that's literally consistent with the utterance\n",
    "        '''\n",
    "        builder_dist = EmptyDistribution()\n",
    "        for action in self.actions :\n",
    "            builder_dist.update({action : 1 if action == lexicon.language_to_dsl(utt) else 0.01})\n",
    "        builder_dist.renormalize()\n",
    "        return builder_dist\n",
    "        \n",
    "    def A0(self, target, lexicon) :\n",
    "        '''\n",
    "        simple architect agent that has equal probability\n",
    "        of saying anything that's literally consistent with the target\n",
    "        '''\n",
    "        architect_dist = EmptyDistribution()\n",
    "        for utt in self.utterances :\n",
    "            architect_dist.update({utt : 1 if utt == lexicon.dsl_to_language(target) else 0.01})\n",
    "        architect_dist.renormalize()\n",
    "        return architect_dist\n",
    "\n",
    "    def update_beliefs(self, previous_trial_df) :\n",
    "        '''\n",
    "        run bayes rule given observations in previous trials\n",
    "        note that we run the calculation in log space because it's more numerically stable\n",
    "        '''\n",
    "        posterior = EmptyDistribution()\n",
    "        posterior.to_logspace()\n",
    "\n",
    "        # for each data point, we calculate the likelihood of the datapoint under each lexicon, \n",
    "        # weighted by the prior probability of that lexicon: \n",
    "        # P(l | obs) \\propto P(l) * \\prod_{o \\in obs} P(o | l)\n",
    "        # log P(l|obs) \\propto log P(l) + \\sum_{o \\in obs} log P(o | l)\n",
    "        for lexicon in self.beliefs.support() :\n",
    "            prior_term = np.log(self.beliefs.score(lexicon))\n",
    "            likelihood_term = sum([\n",
    "                np.log(self.A0(step.target, lexicon).score(step.utterance)) \n",
    "                if self.role == 'builder'\n",
    "                else np.log(self.B0(step.utterance, lexicon).score(step.response))\n",
    "                for i, step in previous_trial_df.iterrows()\n",
    "            ])\n",
    "            posterior.update({lexicon : prior_term + likelihood_term})\n",
    "        posterior.renormalize()\n",
    "        posterior.from_logspace()\n",
    "        self.beliefs = posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4806a",
   "metadata": {},
   "source": [
    "Take a moment to inspect the `LearningAgent` class definition above. In particular, pay special attention to the `update_beliefs()` function which performs the heavy lifting to actually update each agent's beliefs based on what they have observed so far (`previous_trial_df`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbb060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_learning_simulation(d, verbose = False) :\n",
    "    output = pd.DataFrame({\n",
    "        \"utterance\": [], \"response\": [], \"target\": [], \"full_program\" : [], \"target_length\" : [], \"dsl\" : [], \"acc\": []\n",
    "    })\n",
    "    for i, trial in d.iterrows() :\n",
    "        architect = LearningAgent('architect', trial, output) # create agent with updated beliefs\n",
    "        builder = LearningAgent('builder', trial, output)     # create agent with updated beliefs\n",
    "\n",
    "        # random program selected from the options\n",
    "        possiblePrograms = list(trial['programs_with_length'].keys())\n",
    "        target_program = choice(possiblePrograms)\n",
    "\n",
    "        # loop through steps of target program one at a time\n",
    "        target_steps, utts, responses, accs = [], [], [], []\n",
    "        for step in target_program.split(' ') :\n",
    "            # get utterance from architect\n",
    "            utt = architect.act(step)\n",
    "            # get response from builder\n",
    "            response = builder.act(utt)\n",
    "            # update records\n",
    "            target_steps.append(step)\n",
    "            utts.append(utt)\n",
    "            responses.append(response)\n",
    "            accs.append(response == step)\n",
    "\n",
    "        if verbose:\n",
    "            print('beliefs about chunk_C meaning', \n",
    "                  architect.beliefs.marginalize(lambda d : d.dsl_to_language('chunk_C')) \n",
    "                  if 'chunk_C' in trial['dsl'] else None)\n",
    "            print('trial', i)\n",
    "            print(pd.DataFrame({'utts' : utts, 'responses' : responses, 'correct' : accs, 'target' : target_steps}))\n",
    "\n",
    "        output = pd.concat([output, pd.DataFrame({\n",
    "            \"trial\": i,\n",
    "            \"utterance\": utts,\n",
    "            \"response\": responses,\n",
    "            \"acc\": accs,\n",
    "            \"target_steps\" : target_steps,\n",
    "            \"full_program\": target_program,\n",
    "            \"dsl\" : [trial['dsl']] * len(utts),\n",
    "            \"target_length\" : trial['programs_with_length'][target_program],\n",
    "        })])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c93e5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_run_0 = run_learning_simulation(empirical_data, verbose = True)\n",
    "learning_run_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c98008f",
   "metadata": {},
   "source": [
    "## Section 4: Choosing programs.\n",
    "\n",
    "In principle, words allow us to communicate arbitrarily complex concepts, and our mental representation of linguistic meaning is flexible enough to be updated over time. To the extent that both Architects and Builders are learning to 'chunk' blocks over time, each  program fragment) could, in principle, be assigned a new word or phrase, and conveyed directly through the Architects' instructions. Well, almost. There is no guarantee that the words the Architect chooses to pick out a new concept would invoke the same concept in someone else. There is actually uncertainty about how the Builder will interpret the Architects' instructions and this, we suggest, might change what the Architect chooses to say.\n",
    "\n",
    "Our hypothesis is that Architects trade-off communicative *efficiency* with communicative *effectiveness*. While they generally want to say things concisely, if there is too much uncertainty about how the Builder will interpret their words, they will choose a less ambiguous (if more verbose) way of expressing the same information. Concretely, if the Architect wanted to say \"build an L\" but they thought that the Builder wouldn't get what \"L\" meant, they might spell out the steps to make an L-shaped tower instead.\n",
    "\n",
    "Wow, this is great! We can see our agents are updating their beliefs about the lexicon over time and able to get somewhat more accurate as they coordinate. But one of the most interesting things about our empirical data is that speakers seem to be strategically choosing which representation of the tower to convey -- our hypothesis is that the reason participants reduce the length of their utterances over time is that even when new library chunks come online, architects don't always try to refer to them right away. They aren't confident enough that their partner will understand, as the block-level descriptions are much safer. However, the block-level descriptions are also much *costlier* in terms of time and effect because they have to laboriously describe one action at a time. \n",
    "\n",
    "So far, we just used a placeholder for how the speaker picks which representation to communication: they just randomly pick from the list of candidates, slightly preferring shorter programs. However, there are other considerations that ought to go into this decision, namely the estimated likelihood that the listener will do the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019d73b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class StrategicArchitect(LearningAgent) :\n",
    "    def __init__(self, curr_trial, previous_trial_df) :\n",
    "        super().__init__('architect', curr_trial, previous_trial_df) \n",
    "        self.alpha = 1\n",
    "        \n",
    "    def program_utility(self, target_program) :\n",
    "        '''\n",
    "        comptutes the utility of \n",
    "        '''\n",
    "        program_length = len(target_program)\n",
    "        expected_step_infs = []\n",
    "        for step in target_program :\n",
    "            # calculate expected inf(u) = \\sum_L P(L) * ln P_B(a* | u, L) \n",
    "            expected_utt_utility = [\n",
    "                sum([self.beliefs.score(l) * np.log(self.B0(utt, l).score(step))\n",
    "                     for l in self.beliefs.support()])\n",
    "                for utt in self.utterances\n",
    "            ]\n",
    "            # weight inf(u) by how likely softmax speaker is to actually say it\n",
    "            expected_step_infs.append(\n",
    "                sum(expected_utt_utility * softmax(self.alpha * expected_utt_utility))\n",
    "            )\n",
    "        # take average expected informativity at each steps of program (penalizing length)\n",
    "        return np.mean(expected_step_infs) - program_length\n",
    "        \n",
    "    def speak(self, possible_programs) :\n",
    "        '''\n",
    "        produce an action based on role and current beliefs\n",
    "        '''\n",
    "        # Architect is going to build up a distribution over utterances to say\n",
    "        # Architect selects which program representation to comunicate proportional to informativity and length\n",
    "        raw_utilities = [self.program_utility(p.split(' ')) for p in possible_programs]\n",
    "        print(raw_utilities)\n",
    "        print(softmax(self.alpha * raw_utilities))\n",
    "        chosen_p = choice(a = possible_programs, p = softmax(self.alpha * raw_utilities))\n",
    "        print(chosen_p)\n",
    "        return chosen_p, [self.act(step) for step in chosen_p.split(' ')]\n",
    "\n",
    "def run_strategic_simulation(d) :\n",
    "    output = pd.DataFrame({\"utterance\": [], \"response\": [], \"target\": [], \"full_program\" : [], \"target_length\" : [], \"dsl\" : [], \"acc\": []})\n",
    "    for i, trial in d.iterrows() :\n",
    "        architect = StrategicArchitect(trial, output)\n",
    "        builder = LearningAgent('builder', trial, output)\n",
    "\n",
    "        possible_programs = list(trial['programs_with_length'].keys())\n",
    "        chosen_program, utt_seq = architect.speak(possible_programs)\n",
    "\n",
    "        # loop through steps of target program one at a time\n",
    "        target_steps, utts, responses, accs = [], [], [], []\n",
    "        for step, utt in zip(chosen_program.split(' '), utt_seq) :\n",
    "            response = builder.act(utt)\n",
    "            target_steps.append(step)\n",
    "            utts.append(utt)\n",
    "            responses.append(response)\n",
    "            accs.append(response == step)\n",
    "\n",
    "        print('trial', i)\n",
    "        print(pd.DataFrame({'utts' : utts, 'responses' : responses, 'correct' : accs, 'target' : target_steps}))\n",
    "        output = pd.concat([output, pd.DataFrame({\n",
    "            \"utterance\": utts,\n",
    "            \"response\": responses,\n",
    "            \"acc\": accs,\n",
    "            \"target\" : target_steps,\n",
    "            \"full_program\": chosen_program,\n",
    "            \"dsl\" : [trial['dsl']] * len(utts),\n",
    "            \"target_length\" : trial['programs_with_length'][chosen_program],\n",
    "        })])\n",
    "    return output\n",
    "\n",
    "print(run_strategic_simulation(empirical_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604abe4",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "\n",
    "Congratulations! You've finished the tutorial!\n",
    "\n",
    "We hope you found it fun and informative. :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f167e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
